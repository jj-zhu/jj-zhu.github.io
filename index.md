---
layout: page
published: true
---
![jjzhu](/images/jzhu-photo.jpg){:style="float: right;margin-right: 14px;margin-right: 7px;margin-top: 7px;width: 320px;display: block"}
**Update:** *I will be joining the KTH Royal Institute of Technology in Stockholm as an associate professor in mathematics (tenured)*.

I am currently the head of an independent research group at the Weierstrass Institute, Berlin, as well as a PI funded by DFG at TU Darmstadt.
Previously, I worked as a postdoctoral researcher in machine learning at the Max Planck Institute for Intelligent Systems in Tübingen, Germany. My Ph.D. study was in optimization and numerical analysis, at the University of Florida. See [here](/about/) for a short bio. 
I also write [a non-research blog here](https://jj-zhu.github.io/blog/). However, the update frequency depends on how busy I am at the moment.

Overall, I am interested in computational algorithms and dynamical systems.
My group focuses on the mathematical foundations of machine learning and optimization.
In recent years, I have been interested in ML/OPT applications rooted in the principled theory of optimal transport, gradient flows, and kernel methods.

Specifically, I started my research career in optimization and subsequently became interested in **robust machine learning** and **kernel methods**. That requires us to use computational optimization tools that can manipulate probability distributions, which are inherently infinite-dimensional. It led me to my current interests in **variational methods for machine learning** and **optimization over probability distributions**, rooted in the theory of **gradient flows and optimal transport**.

For example, in some of my previous works, I invented [robust probabilistic ML algorithms that can protect against distribution shifts using principled kernel methods](https://arxiv.org/pdf/2006.06981.pdf).
Those optimization algorithms have deep theoretical roots such as the analysis of PDEs.
Following that and after moving to Berlin, I dedicate my current research to interfacing large-scale computational algorithms in machine learning/optimization using PDE gradient flows and optimal transport.
Recently, I became interested in the Hellinger geometry (a.k.a. Fisher-Rao) and collaborated with Alexander Mielke on [kernel methods and (Wasserstein-)Fisher-Rao, a.k.a. (spherical-)Hellinger-Kantorovich, gradient flows](https://jj-zhu.github.io/file/ZhuMielke24AppKerEntFR.pdf).

To get in touch, click the icon at the bottom of the page.
There are sometimes delays in my response to emails, please be patient.

### Upcoming events
- 24 May 2026 - 29 May 2026: [SwissMAP Workshop on "Computational Optimization Meets Gradient Flows and Optimal Transport"](https://swissmaprs.ch/events/computational-optimization-meets-gradient-flows-and-optimal-transport/). Organized by
Niao He (ETH Zurich), Yifan Hu (EPFL), Daniel Kuhn (EPFL), Jia-Jie Zhu (WIAS Berlin)

### Recent talks (selected)
- EPFL Bernoulli workshop “Particles, Flows & Maps for Sampling Complex Distributions”, 2025. Video recording available [here](https://workshop-pfm25.epfl.ch/index.php/program/)
- Gradient Flows Face-to-Face Workshop in Granada, Spain, 2025. Organizers: Maria Bruna, José Alfredo Cañizo, José Antonio Carrillo, Antonio Esposito. Slides available [here](https://wpd.ugr.es/~imag/events/event/gradient-flows/)
- Workshop on Bayesian Analysis and Artificial Intelligence, Peking University, Beijing, China, 2025
- Banff International Research Station (BIRS) "Mathematical Analysis of Adversarial Machine Learning" workshop in Oaxaca, Mexico, from August 17 to August 22, 2025
- The Kantorovich Initiative Seminar, University of British Columbia: [Kernel Approximation of Wasserstein and Fisher-Rao Gradient flows](https://kantorovich.org/event/ki-seminar-zhu/)
- Invited talk [slides for the EPFL talk, Nov 2024](https://jj-zhu.github.io/file/epfl-nov-2024-handout.pdf)
  - [Kernel Approximation of Wasserstein and Fisher-Rao Gradient flows by Prof. Jia-Jie (JJ) Zhu (WIAS Berlin) - EPFL](https://memento.epfl.ch/event/kernel-approximation-of-wasserstein-and-fisher-rao/)
  - [From distributional ambiguity to gradient flows: Wasserstein, Fisher-Rao, and kernel approximation - EPFL](https://memento.epfl.ch/event/from-distributional-ambiguity-to-gradient-flows-wa/)
- Workshop on Optimal Transport and PDEs, Program on The Mathematics of Data, Institute for Mathematical Sciences, National University of Singapore, 2024. Organizers: Philippe Rigollet, Afonso Bandeira, Subhro Ghosh

  

### Open positions
- [Postdoc and PhD positions available] There are upcoming openings in my group. If you are interested in joining, please feel free to inquire.
- Joint PhD position at TU Darmstadt/KTH Royal Institute of Technology (with Jan Peters). See the ad [here](https://jj-zhu.github.io/file/phd_position_advertisement_tuda-kth.pdf)
- Master thesis: if you are a master's student interested in optimization, optimal transport, robust/probabilistic machine learning, generative models and dynamical systems (e.g., diffusion/flow models, neural ODE), please feel free to reach out.

### News and updates
- New publication in the special issue of the journal Communications on Pure and Applied Analysis on the topic of "Transport Equations in Optimization, Sampling, and Control": [Evolution of Gaussians in the Hellinger-Kantorovich-Boltzmann gradient flow](https://arxiv.org/pdf/2504.20400?). Joint work with Matthias Liero, Alexander Mielke, Oliver Tse.
- New publication in *Journal of Optimization Theory and Applications* (JOTA): [An Inexact Halpern Iteration with Application to Distributionally Robust Optimization](https://link.springer.com/article/10.1007/s10957-025-02405-z). Joint work with Ling Liang and Kim-Chuan Toh.

- 28 July - 1 August 2025: [ICSP 2025](https://icsp2025.org/) invited session on interfacing optimal transport, gradient flows, diffusion models, and machine learning, optimization, at École des Ponts, IP Paris. Organized by Pavel Dvurechensky (WIAS Berlin), Jia-Jie Zhu (WIAS Berlin)
  - Speaker slides of the talks are available [here](https://sites.google.com/view/icsp2025invited-ot/home)
- New preprints:
  - [Gradient Flow Sampler-based Distributionally Robust Optimization](https://arxiv.org/abs/2510.25956). Joint work with Zusen Xu. [Code](https://github.com/ZusenXu/GFS-DRO)
  - [Hellinger-Kantorovich Gradient Flows: Global Exponential Decay of Entropy Functionals](https://arxiv.org/abs/2501.17049). Joint work with Alexander Mielke.
  - [Improved Stochastic Optimization of LogSumExp](https://arxiv.org/abs/2509.24894). Joint work with Egor Gladin, Alexey Kroshnin, Pavel Dvurechensky.
  - [Inclusive KL Minimization: A Wasserstein-Fisher-Rao Gradient Flow Perspective. Jia-Jie Zhu](https://arxiv.org/abs/2411.00214)
  - [Kernel Approximation of Fisher-Rao Gradient Flows. Jia-Jie Zhu, Alexander Mielke](https://arxiv.org/abs/2410.20622)
- New work to appear in *NeurIPS 2024*: Globally convergent gradient flows for the MMD-minimization inference problem (a.k.a. MMD-flow). Preprint: [Egor Gladin, Pavel Dvurechensky, Alexander Mielke, Jia-Jie Zhu. Interaction-Force Transport Gradient Flows](https://arxiv.org/abs/2405.17075) Code: [link](https://github.com/egorgladin/ift_flow), 
[Slides (NeurIPS 2024)](https://jj-zhu.github.io/file/IFT-neurips-2024-talk-slides.pdf)
- I'm serving as an area chair for AISTATS 2025. If you are interested in contributing to the community via reviewing a paper, please get in touch.
- Summer 2024. New third-party funding awarded: *DFG Project on "Optimal Transport and Measure Optimization Foundation for Robust and Causal Machine Learning" within the Priority Program “Theoretical Foundations of Deep Learning” (SPP 2298)*.
- March 11th - 15th, 2024. I am organizing a [Workshop on Optimal Transport from Theory to Applications – Interfacing Dynamical Systems, Optimization, and Machine Learning](https://sites.google.com/view/ot-berlin-2024) (OT-DOM) in Berlin, Germany. [program and slides](https://sites.google.com/view/ot-berlin-2024/program-slides?authuser=0)
- New preprints available:
  - [Approximation, Kernelization, and Entropy-Dissipation of Gradient Flows: from Wasserstein to Fisher-Rao](https://jj-zhu.github.io/file/ZhuMielke24AppKerEntFR.pdf). Joint work with Alexander Mielke.
  - [Analysis of Kernel Mirror Prox for Measure Optimization](https://arxiv.org/abs/2403.00147). Accepted for publication at AISTATS 2024. Joint work with Pavel Dvurechensky.
- I am teaching the [nonparametric statistics course at Humboldt University of Berlin (at master level)](https://agnes.hu-berlin.de/lupo/rds?state=verpublish&status=init&vmfile=no&publishid=207589&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung), co-lecturing with Vladimir Spokoinyi, in term 2023/24.
- Recent talks on gradient flow force-balance, especially in robust learning under (strong) structured distribution shifts, and conditional moment restriction for causal inference at 
  - [EUCCO 2023](https://scoop.iwr.uni-heidelberg.de/events/2023_eucco/), at Heidelberg University. [Talk slides available](https://jj-zhu.github.io/file/Heidelberg-EUCCO-2023-Zhu.pdf)
- I taught a mini-course on the optimization perspective of gradient flow dynamics, introducing (beginner-friendly) concepts of gradient flows in the Eulidean and Wasserstein space, at the [Workshop of Intelligent Autonomous Learning Systems 2023](https://www.ias.informatik.tu-darmstadt.de/Workshops/IWIALS2023). Lecture slides available:  
  - [slides for the mini-course](https://jj-zhu.github.io/file/IWIAS-mini-course-opt-gf-aug-2023-nopause.pdf)
  - [slides for the introduction to our research](https://jj-zhu.github.io/file/IWIAS-2023-intro-zhu.pdf)
- July 2023. I gave an invited talk at the [ICML 2023 Workshop on Duality Principles for Modern Machine Learning](https://dp4ml.github.io/). The slides are available [here](https://jj-zhu.github.io/file/duality-ICML-2023-Zhu.pdf).
- July 2023. Accepted paper at CDC 2023: [Propagating Kernel Ambiguity Sets in Nonlinear Data-driven Dynamics Models](https://arxiv.org/abs/2304.14057)
- May 2023. Accepted paper at ICML 2023 [(link to preprint)](https://arxiv.org/abs/2305.10898): Heiner Kremer, Yassine Nemmour, Bernhard Sch ̈olkopf, and Jia-Jie Zhu. Estimation Beyond Data Reweighting: Kernel Method of Moments.
- May 2023. A couple of new preprints are available:
    - [Nonlinear Wasserstein Distributionally Robust Optimal Control](https://arxiv.org/abs/2304.07415)
    - [Propagating Kernel Ambiguity Sets in Nonlinear Data-driven Dynamics Models](https://arxiv.org/abs/2304.14057)
- Apr 2023. Gave a plenary talk at the Leibniz Institute for Agricultural Engineering and Bioeconomy Potsdam, during the workshop "Mathematical Modeling and Simulation" (MMS) Days.
- Served as area chair for AISTATS 2023.  


### Twitter feed
<a class="twitter-timeline" data-width="800" href="https://twitter.com/__jzhu__?ref_src=twsrc%5Etfw">Tweets by __jzhu__</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
